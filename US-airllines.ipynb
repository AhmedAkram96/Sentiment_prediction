{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US-Airlines Dataset\n",
    "\n",
    "\n",
    "\n",
    "This Jupyter notebook includes Dataset collected from the tweets of passengers about their flights with an American airline.\n",
    "\n",
    "### So, in this notebook our aim is to predict these tweets sentiment, whther it's Positive, Negative or Neutral.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " At first, We start by fetching and inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513  neutral           1.0000                         \n",
       "1  570301130888122368  positive          0.3486                         \n",
       "2  570301083672813571  neutral           0.6837                         \n",
       "3  570301031407624196  negative          1.0000                         \n",
       "4  570300817074462722  negative          1.0000                         \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0  NaN           NaN                         Virgin America   \n",
       "1  NaN            0.0000                     Virgin America   \n",
       "2  NaN           NaN                         Virgin America   \n",
       "3  Bad Flight     0.7033                     Virgin America   \n",
       "4  Can't Tell     1.0000                     Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0  NaN                    cairdin     NaN                 0               \n",
       "1  NaN                    jnardino    NaN                 0               \n",
       "2  NaN                    yvonnalynn  NaN                 0               \n",
       "3  NaN                    jnardino    NaN                 0               \n",
       "4  NaN                    jnardino    NaN                 0               \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0  NaN         2015-02-24 11:35:52 -0800  NaN             \n",
       "1  NaN         2015-02-24 11:15:59 -0800  NaN             \n",
       "2  NaN         2015-02-24 11:15:48 -0800  Lets Play       \n",
       "3  NaN         2015-02-24 11:15:36 -0800  NaN             \n",
       "4  NaN         2015-02-24 11:14:45 -0800  NaN             \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting the data\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We investigate our dataset important attributes, In our case It's the Text attribute as Data and the Sentiment attribute as the label</b>\n",
    "\n",
    "Start with the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count = tweets.airline_sentiment.value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Then, We remove all the duplicates</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop_duplicates(keep='first',subset='tweet_id') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    9082\n",
      "neutral     3069\n",
      "positive    2334\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiment_counts = tweets.airline_sentiment.value_counts()\n",
    "number_of_tweets = tweets.tweet_id.count()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> As you can see above, the values for each sentiment (Class) are not equal, hence, the dataset is not balanced.\n",
    "Applying a model to unbalanced dataset results in a biased model toward the Class with most instances.\n",
    "\n",
    "So, We have to balance the dataset, using Resampling. We chose Upsampling instead of downsampling because\n",
    "We Don't have a lot of data. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     9082\n",
       "negative    9082\n",
       "positive    9082\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "# Separate majority and minority classes\n",
    "majority_class = tweets.loc[tweets.airline_sentiment==\"negative\"]\n",
    "minority1_class = tweets.loc[(tweets.airline_sentiment==\"neutral\")]\n",
    "minority2_class = tweets.loc[(tweets.airline_sentiment==\"positive\")] \n",
    "\n",
    "# Upsample minority class\n",
    "minority_upsampled1 = resample(minority1_class, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=9082) # majority instances\n",
    "# Upsample minority class\n",
    "minority_upsampled2 = resample(minority2_class, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=9082) # majority instances\n",
    " \n",
    "# Combine majority class with upsampled minority classes\n",
    "upsampled = pd.concat([majority_class, minority_upsampled1, minority_upsampled2])\n",
    " \n",
    "# Display new class counts\n",
    "upsampled.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Here comes the part where we need to prepare the data to be ready for the training.</b>\n",
    "\n",
    "We need to download some plugins from the Natural language ToolKit to help us clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We create a Function that will clean the data as follows:</b>\n",
    "1. Remove all the Symbols and keep the letters only (Remove @ at the beginning)\n",
    "2. Remove the name of the Airline at the beginning of the Text\n",
    "3. Set it all to lowercase\n",
    "4. Remove all the Englosh stopping words\n",
    "5. Lemmatize all the words (Get the original words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalizer(tweet):\n",
    "    only_letters = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n",
    "    tokens = nltk.word_tokenize(only_letters)[2:]\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to the Text attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>[really, aggressive, blast, obnoxious, entertainment, guest, face, amp, little, recourse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>[really, big, bad, thing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA</td>\n",
       "      <td>[would, pay, flight, seat, playing, really, bad, thing, flying, va]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica SFO-PDX schedule is still MIA.</td>\n",
       "      <td>[pdx, schedule, still, mia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@VirginAmerica  I flew from NYC to SFO last week and couldn't fully sit in my seat due to two large gentleman on either side of me. HELP!</td>\n",
       "      <td>[flew, nyc, sfo, last, week, fully, sit, seat, due, two, large, gentleman, either, side, help]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         text  \\\n",
       "3   @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse              \n",
       "4   @VirginAmerica and it's a really big bad thing about it                                                                                     \n",
       "5   @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA    \n",
       "15  @VirginAmerica SFO-PDX schedule is still MIA.                                                                                               \n",
       "17  @VirginAmerica  I flew from NYC to SFO last week and couldn't fully sit in my seat due to two large gentleman on either side of me. HELP!   \n",
       "\n",
       "                                                                                  normalized_tweet  \n",
       "3   [really, aggressive, blast, obnoxious, entertainment, guest, face, amp, little, recourse]       \n",
       "4   [really, big, bad, thing]                                                                       \n",
       "5   [would, pay, flight, seat, playing, really, bad, thing, flying, va]                             \n",
       "15  [pdx, schedule, still, mia]                                                                     \n",
       "17  [flew, nyc, sfo, last, week, fully, sit, seat, due, two, large, gentleman, either, side, help]  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1) # Setting this so we can see the full content of cells\n",
    "upsampled['normalized_tweet'] = upsampled.text.apply(normalizer)\n",
    "upsampled[['text','normalized_tweet']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Finally, We need to vectorize the text words into integer representation to help to the model to understand the data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We need to convert the Normalized tweets from list objects into strings, so the vectorizer would be able to deal with it </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3        really aggressive blast obnoxious entertainment guest face amp little recourse             \n",
       "4        really big bad thing                                                                       \n",
       "5        would pay flight seat playing really bad thing flying va                                   \n",
       "15       pdx schedule still mia                                                                     \n",
       "17       flew nyc sfo last week fully sit seat due two large gentleman either side help             \n",
       "20       first fare may three time carrier seat available select                                    \n",
       "24       guy messed seating reserved seating friend guy gave seat away want free internet           \n",
       "25       match program applied three week called emailed response                                   \n",
       "26       happened ur vegan food option least say ur site know able eat anything next hr fail        \n",
       "28       get cold air vent vx noair worstflightever roasted sfotobos                                \n",
       "30       bked cool birthday trip add elevate cause entered middle name flight booking problem       \n",
       "32       left expensive headphone flight iad lax today seat one answering l amp f number lax        \n",
       "33       return phone call would prefer use online self service option                              \n",
       "39       chat support working site http co vhp gtdwpk                                               \n",
       "41       first time flyer next week excited hard time getting flight added elevate account help     \n",
       "55       excited lga gt dal deal trying book since last week amp page never load thx                \n",
       "61       called week ago adding flight elevate still shown help                                     \n",
       "66       guyyyys trying get hour someone call please                                                \n",
       "67       virgin hold minute earlier flight la nyc tonight earlier pm                                \n",
       "69       fine lost bag                                                                              \n",
       "73       airline awesome lax loft need step game dirty table floor http co hy vrfhjht               \n",
       "78       going customer service anyway speak human asap thank                                       \n",
       "80       supp biz traveler like southwestair customer service like jetblue neverflyvirginforbusiness\n",
       "82       best whenever begrudgingly use airline delayed late flight                                 \n",
       "83       interesting flying cancelled flight next four flight planned neverflyvirginforbusiness     \n",
       "84       disappointing experience shared every business traveler meet neverflyvirgin                \n",
       "85       trouble adding flight wife booked elevate account help http co px hqoks r                  \n",
       "89       site back                                                                                  \n",
       "92       like tv interesting video disappointed cancelled flightled flight flight went jfk saturday \n",
       "93       landed lax hour late flight bag check business travel friendly nomorevirgin                \n",
       "                                            ...                                                     \n",
       "7791     couple pgashow flight til mosaic wait easier change flight weather bad jetblue             \n",
       "4543     gate c denver international fantastic customer service helping new flyer thx amazing staff \n",
       "7917     love jetblue speedy twitter customer service                                               \n",
       "1928     leaving year old row flight lax iad                                                        \n",
       "4357     bc system outage hopefully everything go smoothly thank follow                             \n",
       "6388     thanks team family emerg day ticket experience excellent st time flying yall last          \n",
       "13442                                                                                               \n",
       "11171    sju check fabulous                                                                         \n",
       "10881    agent phone great job                                                                      \n",
       "459      would rough trip luckily virginamerica flight weather                                      \n",
       "4567                                                                                                \n",
       "5760     week bringing blanket bc freezing                                                          \n",
       "8264     thanks                                                                                     \n",
       "9943     marie lga best ticket agent ever excellentcustomerservice                                  \n",
       "994                                                                                                 \n",
       "7937     info figured case hopefully new hashtag change abcletjetbluestreamfeed                     \n",
       "7738     plane hopefully longer mercy playlist thanks                                               \n",
       "4927     beautiful fleet perfect evening fly http co xmz tf ix                                      \n",
       "12406                                                                                               \n",
       "11669    high thanks                                                                                \n",
       "8275     reimbursed everyone flight portion ticket still love jetblue best american airline         \n",
       "4022     chicago hometown airline care neighbor savethediagonals flyquiet ordnoise                  \n",
       "4772     one plane thanks taking arizona http co finq fh ue                                         \n",
       "4113                                                                                                \n",
       "6652     ya sing song finally get plane back nashville lovesouthwestair                             \n",
       "11595    best airway follow please                                                                  \n",
       "5571                                                                                                \n",
       "12380    great customer service family made back sat safely weather dfw made thing little worrisome \n",
       "7453     great experience flight sfo jfk seat service food everything top quality back soon         \n",
       "6618     worry best u already cancelled flighted biz trip still luv rr                              \n",
       "Name: normalized_tweet, Length: 27246, dtype: object"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled.normalized_tweet = upsampled.normalized_tweet.tolist()\n",
    "\n",
    "new_tweets = []\n",
    "for tweet in upsampled.normalized_tweet:\n",
    "    newtweet = \" \"\n",
    "    new_tweets.append(newtweet.join(tweet))\n",
    "upsampled.normalized_tweet = new_tweets\n",
    "upsampled.normalized_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Apply the vectorizer </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = count_vectorizer.fit_transform(upsampled.normalized_tweet)\n",
    "indexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Change the targets from text into integer representation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment2target(sentiment):\n",
    "    return {\n",
    "        'negative': 0,\n",
    "        'neutral': 1,\n",
    "        'positive' : 2\n",
    "    }[sentiment]\n",
    "targets = upsampled.airline_sentiment.apply(sentiment2target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Here we came to the part of modeling our dataset, We suggest using SVM and RandomForrest as classifiers for our 3 classes </b>\n",
    "\n",
    "First, we split the dataset into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.4, random_state=0)\n",
    "data_train_index = data_train[:,0]\n",
    "data_train = data_train[:,1:]\n",
    "data_test_index = data_test[:,0]\n",
    "data_test = data_test[:,1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine proved it's effieiciency in classification data with small number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf1 = svm.SVC(gamma=0.01, C=100., probability=True, class_weight='balanced', kernel='linear')\n",
    "clf1_output = clf1.fit(data_train, targets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have reached the part of testing our model effeciency:\n",
    "After a lot of thinking, we found that <b>accuracy</b> is the best metrics to evaluate this model. We don't need <b>precision or recall</b> because we don't prefer that all positive be true and some negative be mistaken as positive and we don't prefer the opposite, each prediction is important to us whther it's positively or negatively predicted, which leaves us with <b>F1 score or accuracy</b>. F1 would have been perfect if our dataset is not balanced, which is not the case in our dataset after doing the upsampling. So <b>accuracy</b> is the perfect metric for our problem, besides, we are going to draw a confusion matrix to examine the miss-classified instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8624644462794752"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(data_test, targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Now let's try some Random examples </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = count_vectorizer.transform([\n",
    "    \"This is a great flight I had so Much fun\",\n",
    "    \"Thank you, The trip was great\",\n",
    "    \"this is the worst trip I've ever had in my life, So bad\",\n",
    "    \"Service was awful. I'll never fly with you again.\",\n",
    "    \"I lost my luggage !! Don't fly with this shitty airline again\",\n",
    "    \"I have no opinion,no bad or good feelings, this flight is normal\",\n",
    "])\n",
    "prediction = clf1.predict_proba(sentences)\n",
    "\n",
    "indecies = prediction.argmax(axis=1)\n",
    "labels = []\n",
    "\n",
    "for index in indecies:\n",
    "    if index == 2:\n",
    "        labels.append(\"positive\")\n",
    "    else:\n",
    "        if index == 1:\n",
    "            labels.append(\"neutral\")\n",
    "        else:\n",
    "            labels.append(\"negative\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'positive', 'negative', 'negative', 'negative', 'positive']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can otice that our classifier predicted all the results successfully, But why There is still some miss-predictions with almost 15% of the data? \n",
    " \n",
    "We can draw the confusion matrix to know from where exactly this missclassification is coming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf860401d0>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction1 = clf1.predict_proba(data_test)\n",
    "prediction1 = prediction1.argmax(axis=1)\n",
    "target_list = targets_test.tolist()\n",
    "\n",
    "\n",
    "Confusion_Matrix =[x[:] for x in [[0] * 3] * 3]\n",
    "\n",
    "for i in range(len(target_list)):\n",
    "    temp = int(target_list[i])\n",
    "    Confusion_Matrix[temp][prediction1[i]] = Confusion_Matrix[temp][prediction1[i]]+1\n",
    "\n",
    "final = np.asarray(Confusion_Matrix)\n",
    "\n",
    "plt.imsave(\"Confusion.jpg\",final)\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(Confusion_Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> From the confusion matrix, We can notice that many Negative class instances is classified as neutral and vice versa, and the positive class is almost perfect. This shows us the effectivness of the Upsampling we did at the phase of the data Engineering part to the positive class instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest because it's highly robust to outliers, which will help avoid the class from being misleaded in classifying repeated words in different classes\n",
    "\n",
    "imagne a word that appears many times in one class and few other times in other class.It originally belongs to the 1st class, and it's just an outlier in the 2nd class, Random Forest will be very helpful in avoiding this outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=500,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=0, verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   n_jobs=None)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "clf2 = OneVsOneClassifier(RandomForestClassifier(max_depth=500, random_state=0))\n",
    "clf2.fit(data_train, targets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8781539590788145"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(data_test, targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = count_vectorizer.transform([\n",
    "    \"This is a great flight I had so Much fun\",\n",
    "    \"Thank you, The trip was great\",\n",
    "    \"this is the worst trip I've ever had in my life\",\n",
    "    \"Service was awful. I'll never fly with you again.\",\n",
    "    \"I lost my luggage !! Don't fly with this shitty airline again\",\n",
    "    \"I have no opinion,no bad or good feelings, this flight is normal\",\n",
    "])\n",
    "prediction = clf2.predict(sentences)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "indecies = prediction\n",
    "labels = []\n",
    "\n",
    "for index in indecies:\n",
    "    if index == 2:\n",
    "        labels.append(\"positive\")\n",
    "    else:\n",
    "        if index == 1:\n",
    "            labels.append(\"neutral\")\n",
    "        else:\n",
    "            labels.append(\"negative\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'positive', 'negative', 'negative', 'negative', 'neutral']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf85f38b00>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction1 = clf2.predict(data_test)\n",
    "\n",
    "target_list = targets_test.tolist()\n",
    "\n",
    "\n",
    "Confusion_Matrix =[x[:] for x in [[0] * 3] * 3]\n",
    "\n",
    "for i in range(len(target_list)):\n",
    "    temp = int(target_list[i])\n",
    "    Confusion_Matrix[temp][prediction1[i]] = Confusion_Matrix[temp][prediction1[i]]+1\n",
    "\n",
    "final = np.asarray(Confusion_Matrix)\n",
    "\n",
    "plt.imsave(\"Confusion.jpg\",final)\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(Confusion_Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> From the confusion matrix, as the SVM,  We can notice that many Negative class instances is classified as neutral and vice versa But quite less than SVM, which shows some improvement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Finally, The weaknesses in this model is that some words are repeated in many classes of this dataset which makes it hard for the Machine learning model to classify it into the right class. Maybe Some extra data will enhance the results, because even if some words are repeated in many classes, of course it's going to be repeated more in the right class.\n",
    "\n",
    "If we managed to gain some more data, at this point we can use Naive bayes for Multinomial. Because it's very good in classifying text datasets, as it assumes independence among data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
